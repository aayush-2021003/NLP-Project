# NLP-Project
NLP Course Project 2024
# Abstract
Human Value Detection, which is Task 4 for SemEval 2023, aims to predict the human values from a set of 20 value categories in a given argument. The nuanced, implicit, and often subjective nature of value categories in textual arguments makes this a difficult task. The slightly small size of the dataset and lack of label awareness add an added challenge. To address these challenges we propose a model which draws on Textual Entailment and Contrastive Learning. The model achieves an F1 score of 0.5681, which beats the best submission in SemEval 23.
# Result
We have compiled the results in Table 1. We have evaluated our code on four datasets (provided by the organiser): 1) Validation set obtained from the training set itself 2) Testing set 3) Nahj-al-Balagha, an Islamic religious text 4) New York Times The following results show that our model with a test set F1 score of 0.5681 beats the previous best F1 of 0.561 set by Adam Smith. We test the robustness of the model using the Nahj-al-Balagha dataset and find the model gets an F1 of 0.46 more than the best F1 of 0.36(Adam Smith). The accuracy, precision and recall on the same dataset were 0.64,0.54 and 0.62 respectively. This shows the robustness of our model to unseen data. We also analyse the F1 score for each value category and find the model fares poorly for Conformity-rules(0.39) and benevolence-caring(0.40). These labels were in a minority in the training set which could suggest the poor performance. The explanation model mostly produced highly fluent explanations but was only very average in the adequacy. At times, the model’s explanations, while displaying a high degree of fluency, showed a divergence in the adequacy of content. This occasional discrepancy between the value labels predicted by the classification model and the subsequent explanations may account for the observed variation in adequacy. The inter-annotator agreement on the quality of adequacy measured using Cohen’s Kappa was calculated to be 0.7736 among two annotators. The fluency was by and large mostly good and consistent across different samples. The Cohen’s Kappa for fluency was not calculated due to paucity of time.
