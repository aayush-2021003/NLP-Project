{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8194132,"sourceType":"datasetVersion","datasetId":4853226},{"sourceId":8194151,"sourceType":"datasetVersion","datasetId":4853242}],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-04-23T19:13:30.750491Z","iopub.execute_input":"2024-04-23T19:13:30.750864Z","iopub.status.idle":"2024-04-23T19:13:31.144337Z","shell.execute_reply.started":"2024-04-23T19:13:30.750830Z","shell.execute_reply":"2024-04-23T19:13:31.143382Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/projectvalues/Project_Data/labels-test.tsv\n/kaggle/input/projectvalues/Project_Data/arguments-validation.tsv\n/kaggle/input/projectvalues/Project_Data/arguments-training.tsv\n/kaggle/input/projectvalues/Project_Data/README.md\n/kaggle/input/projectvalues/Project_Data/arguments-test.tsv\n/kaggle/input/projectvalues/Project_Data/labels-validation.tsv\n/kaggle/input/projectvalues/Project_Data/labels-training.tsv\n/kaggle/input/value-categories/value-categories (1).json\n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\nimport json\n\n# Load the arguments data (assuming it has columns 'id', 'premise')\narguments_df = pd.read_csv('/kaggle/input/projectvalues/Project_Data/arguments-training.tsv', delimiter='\\t')\n\n# Load the value labels data (assuming it has columns 'id', 'value_label')\nlabels_df = pd.read_csv('/kaggle/input/projectvalues/Project_Data/labels-training.tsv', delimiter='\\t')\n\n# Load the value descriptions from a JSON file\nwith open('/kaggle/input/value-categories/value-categories (1).json', 'r') as file:\n    value_descriptions = json.load(file)\n    \nlabels_long_df = labels_df.melt(id_vars='Argument ID', var_name='value_category', value_name='label')\nlabels_long_df = labels_long_df[labels_long_df['value_category'] != 'Universalism: objectivity']\nlabels_long_df['value_description'] = labels_long_df['value_category'].apply(lambda x: value_descriptions[x.lower().replace(\": \",\"-\")]['personal-motivation'])\ncombined_df = pd.merge(arguments_df, labels_long_df, left_on='Argument ID', right_on='Argument ID')\ncombined_df['Argument'] = combined_df.apply(\n    lambda row: f\"{row['Stance']} {row['Conclusion']} by saying {row['Premise']}\",\n    axis=1\n)","metadata":{"execution":{"iopub.status.busy":"2024-04-23T19:13:31.146251Z","iopub.execute_input":"2024-04-23T19:13:31.146644Z","iopub.status.idle":"2024-04-23T19:13:33.277617Z","shell.execute_reply.started":"2024-04-23T19:13:31.146619Z","shell.execute_reply":"2024-04-23T19:13:33.276504Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"final_df=combined_df\ndf_majority = final_df[final_df.label == 0]\ndf_minority = final_df[final_df.label == 1]\n\n# Determine the number of instances you want to keep from the majority class\n# For example, you might want to have a 1:1 ratio\nnumber_of_instances = len(df_minority)\n\n# Downsample the majority class\ndf_majority_downsampled = df_majority.sample(n=number_of_instances)\n\n# Combine the downsampled majority class with the minority class to get a balanced dataset\nbalanced_df = pd.concat([df_majority_downsampled, df_minority])\n\n# Shuffle the dataset to mix the two classes well\nbalanced_df = balanced_df.sample(frac=1).reset_index(drop=True)","metadata":{"execution":{"iopub.status.busy":"2024-04-23T19:13:33.279370Z","iopub.execute_input":"2024-04-23T19:13:33.279788Z","iopub.status.idle":"2024-04-23T19:13:33.327747Z","shell.execute_reply.started":"2024-04-23T19:13:33.279753Z","shell.execute_reply":"2024-04-23T19:13:33.326982Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"balanced_df","metadata":{"execution":{"iopub.status.busy":"2024-04-23T19:13:33.329737Z","iopub.execute_input":"2024-04-23T19:13:33.330053Z","iopub.status.idle":"2024-04-23T19:13:33.347725Z","shell.execute_reply.started":"2024-04-23T19:13:33.330027Z","shell.execute_reply":"2024-04-23T19:13:33.346807Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"      Argument ID                                 Conclusion       Stance  \\\n0          A04002      We should subsidize space exploration      against   \n1          A03002            We should legalize prostitution      against   \n2          A12174   We should abolish the three-strikes laws  in favor of   \n3          A29011   We should abolish the three-strikes laws      against   \n4          A20441          We should limit judicial activism  in favor of   \n...           ...                                        ...          ...   \n34627      A27263  We should ban cosmetic surgery for minors      against   \n34628      A28035  We should ban cosmetic surgery for minors      against   \n34629      A18420      We should subsidize space exploration  in favor of   \n34630      D01032    Technology is making us feel less human  in favor of   \n34631      A24087              We should ban factory farming  in favor of   \n\n                                                 Premise  \\\n0      there are more important things to spend publi...   \n1      legalizing prostitution will further demean a ...   \n2      it is unfair to convict someone for a third cr...   \n3      we need to take control of criminals and hold ...   \n4      judicial activism should be limited to avoid v...   \n...                                                  ...   \n34627  sometimes cosmetic surgery isn't so much cosme...   \n34628  some children may require cosmetic surgery for...   \n34629  subsidizing space exploration is a positive mo...   \n34630  We are increasingly depending on technological...   \n34631  factory farming provides negative outcomes acr...   \n\n                value_category  label  \\\n0            Conformity: rules      0   \n1         Universalism: nature      0   \n2        Universalism: concern      1   \n3                     Hedonism      0   \n4           Security: societal      1   \n...                        ...    ...   \n34627  Universalism: tolerance      1   \n34628       Security: personal      1   \n34629    Universalism: concern      0   \n34630       Security: personal      0   \n34631         Power: resources      0   \n\n                                       value_description  \\\n0      Should follow authorities, follow rules even i...   \n1      Care about nature for nature's sake, protect t...   \n2      Protecting the weak and vulnerable, care about...   \n3      Having a good time, enjoying lifeâ€™s pleasures ...   \n4      Country should protect itself against all thre...   \n...                                                  ...   \n34627  Care about peace and harmony, listen to people...   \n34628  Avoid dangerous situations, value personal sec...   \n34629  Protecting the weak and vulnerable, care about...   \n34630  Avoid dangerous situations, value personal sec...   \n34631  Having lots of money for the power it brings, ...   \n\n                                                Argument  \n0      against We should subsidize space exploration ...  \n1      against We should legalize prostitution by say...  \n2      in favor of We should abolish the three-strike...  \n3      against We should abolish the three-strikes la...  \n4      in favor of We should limit judicial activism ...  \n...                                                  ...  \n34627  against We should ban cosmetic surgery for min...  \n34628  against We should ban cosmetic surgery for min...  \n34629  in favor of We should subsidize space explorat...  \n34630  in favor of Technology is making us feel less ...  \n34631  in favor of We should ban factory farming by s...  \n\n[34632 rows x 8 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Argument ID</th>\n      <th>Conclusion</th>\n      <th>Stance</th>\n      <th>Premise</th>\n      <th>value_category</th>\n      <th>label</th>\n      <th>value_description</th>\n      <th>Argument</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>A04002</td>\n      <td>We should subsidize space exploration</td>\n      <td>against</td>\n      <td>there are more important things to spend publi...</td>\n      <td>Conformity: rules</td>\n      <td>0</td>\n      <td>Should follow authorities, follow rules even i...</td>\n      <td>against We should subsidize space exploration ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>A03002</td>\n      <td>We should legalize prostitution</td>\n      <td>against</td>\n      <td>legalizing prostitution will further demean a ...</td>\n      <td>Universalism: nature</td>\n      <td>0</td>\n      <td>Care about nature for nature's sake, protect t...</td>\n      <td>against We should legalize prostitution by say...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>A12174</td>\n      <td>We should abolish the three-strikes laws</td>\n      <td>in favor of</td>\n      <td>it is unfair to convict someone for a third cr...</td>\n      <td>Universalism: concern</td>\n      <td>1</td>\n      <td>Protecting the weak and vulnerable, care about...</td>\n      <td>in favor of We should abolish the three-strike...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>A29011</td>\n      <td>We should abolish the three-strikes laws</td>\n      <td>against</td>\n      <td>we need to take control of criminals and hold ...</td>\n      <td>Hedonism</td>\n      <td>0</td>\n      <td>Having a good time, enjoying lifeâ€™s pleasures ...</td>\n      <td>against We should abolish the three-strikes la...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>A20441</td>\n      <td>We should limit judicial activism</td>\n      <td>in favor of</td>\n      <td>judicial activism should be limited to avoid v...</td>\n      <td>Security: societal</td>\n      <td>1</td>\n      <td>Country should protect itself against all thre...</td>\n      <td>in favor of We should limit judicial activism ...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>34627</th>\n      <td>A27263</td>\n      <td>We should ban cosmetic surgery for minors</td>\n      <td>against</td>\n      <td>sometimes cosmetic surgery isn't so much cosme...</td>\n      <td>Universalism: tolerance</td>\n      <td>1</td>\n      <td>Care about peace and harmony, listen to people...</td>\n      <td>against We should ban cosmetic surgery for min...</td>\n    </tr>\n    <tr>\n      <th>34628</th>\n      <td>A28035</td>\n      <td>We should ban cosmetic surgery for minors</td>\n      <td>against</td>\n      <td>some children may require cosmetic surgery for...</td>\n      <td>Security: personal</td>\n      <td>1</td>\n      <td>Avoid dangerous situations, value personal sec...</td>\n      <td>against We should ban cosmetic surgery for min...</td>\n    </tr>\n    <tr>\n      <th>34629</th>\n      <td>A18420</td>\n      <td>We should subsidize space exploration</td>\n      <td>in favor of</td>\n      <td>subsidizing space exploration is a positive mo...</td>\n      <td>Universalism: concern</td>\n      <td>0</td>\n      <td>Protecting the weak and vulnerable, care about...</td>\n      <td>in favor of We should subsidize space explorat...</td>\n    </tr>\n    <tr>\n      <th>34630</th>\n      <td>D01032</td>\n      <td>Technology is making us feel less human</td>\n      <td>in favor of</td>\n      <td>We are increasingly depending on technological...</td>\n      <td>Security: personal</td>\n      <td>0</td>\n      <td>Avoid dangerous situations, value personal sec...</td>\n      <td>in favor of Technology is making us feel less ...</td>\n    </tr>\n    <tr>\n      <th>34631</th>\n      <td>A24087</td>\n      <td>We should ban factory farming</td>\n      <td>in favor of</td>\n      <td>factory farming provides negative outcomes acr...</td>\n      <td>Power: resources</td>\n      <td>0</td>\n      <td>Having lots of money for the power it brings, ...</td>\n      <td>in favor of We should ban factory farming by s...</td>\n    </tr>\n  </tbody>\n</table>\n<p>34632 rows Ã— 8 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"from torch.utils.data import DataLoader, TensorDataset, random_split\nfrom transformers import AdamW, get_linear_schedule_with_warmup\nfrom torch.nn.functional import cross_entropy\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer\nimport torch\nfrom sklearn.metrics import accuracy_score,f1_score\nmodel_name = 'pepa/roberta-base-snli'\nconfig = AutoModelForSequenceClassification.from_pretrained(model_name, \n                                                             return_dict=True,\n                                                             output_hidden_states=False,\n                                                             hidden_dropout_prob=0.3,  # Set dropout probability for hidden layers\n                                                             attention_probs_dropout_prob=0.3)  # Set dropout probability for attention layers\n\n\n# Verify that we are only optimizing biases\n\nmodel = AutoModelForSequenceClassification.from_pretrained(model_name, config=config)\nbias_parameters = []\nfor name, param in model.named_parameters():\n    if 'bias' in name:\n        bias_parameters.append(param)\n    else:\n        # Set requires_grad to False to freeze all non-bias parameters\n        param.requires_grad = False\nprint(f\"Total trainable parameters: {len(bias_parameters)}\")\ntokenizer = AutoTokenizer.from_pretrained(model_name)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel=model.to(device)\ninputs = tokenizer(list(balanced_df['Argument']), list(balanced_df['value_description']), padding=True, truncation=True, return_tensors=\"pt\")\ninput_ids = inputs['input_ids']\nattention_mask = inputs['attention_mask']\nlabels = torch.tensor(balanced_df['label'].values)","metadata":{"execution":{"iopub.status.busy":"2024-04-23T19:13:33.348992Z","iopub.execute_input":"2024-04-23T19:13:33.349318Z","iopub.status.idle":"2024-04-23T19:13:55.498604Z","shell.execute_reply.started":"2024-04-23T19:13:33.349287Z","shell.execute_reply":"2024-04-23T19:13:55.497600Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n  return self.fget.__get__(instance, owner)()\n","output_type":"stream"},{"name":"stdout","text":"Total trainable parameters: 99\n","output_type":"stream"}]},{"cell_type":"code","source":"train_dataset = TensorDataset(input_ids, attention_mask, labels)\ntrain_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=8)\noptimizer = AdamW(bias_parameters, lr=2e-5)\nepochs=1\ntotal_steps = len(train_dataloader) * epochs\nscheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\nimport numpy as np\n# Define a training loop\nfor epoch in range(epochs):\n    model.train()\n    total_loss = 0\n    predictions = []\n    true_labels = []\n    steps=1\n    i=0\n    for batch in train_dataloader:\n        b_input_ids, b_attention_mask, b_labels = batch\n        b_input_ids = b_input_ids.to(device)\n        b_attention_mask = b_attention_mask.to(device)\n        b_labels = b_labels.to(device)\n        \n        # Clear any previously calculated gradients\n        optimizer.zero_grad()\n        \n        # Perform a forward pass. This will return logits.\n        outputs = model(b_input_ids, attention_mask=b_attention_mask, labels=b_labels)\n        \n        # Calculate loss using the outputs and the labels\n        loss = outputs[0]\n        total_loss += loss.item()\n        logits = outputs.logits.detach().cpu().numpy()\n        label_ids = b_labels.to('cpu').numpy()\n        predictions.extend(np.argmax(logits, axis=1).flatten())\n        true_labels.extend(label_ids.flatten())\n        \n        # Perform a backward pass to calculate gradients\n        loss.backward()\n        \n        # Update parameters and take a step using the computed gradient\n        optimizer.step()\n        \n        # Update the learning rate\n        scheduler.step()\n        if steps % 200 == 0:\n            interim_f1 = f1_score(true_labels, predictions, average='macro')\n            print(f\"Epoch {epoch}, Step {steps}, Loss: {total_loss / steps:.4f}, Interim F1 Score: {interim_f1:.4f}\")\n            predictions = []  # Reset predictions\n            true_labels = []  # Reset true labels\n        steps+=1\n    \n    # Calculate the average loss over the training data\n    avg_train_loss = total_loss / len(train_dataloader)\n\n\nprint(\"Training complete\")","metadata":{"execution":{"iopub.status.busy":"2024-04-23T19:13:55.499947Z","iopub.execute_input":"2024-04-23T19:13:55.500422Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:457: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Epoch 0, Step 200, Loss: 1.2172, Interim F1 Score: 0.2503\nEpoch 0, Step 400, Loss: 1.1008, Interim F1 Score: 0.2741\nEpoch 0, Step 600, Loss: 1.0375, Interim F1 Score: 0.2693\nEpoch 0, Step 800, Loss: 0.9963, Interim F1 Score: 0.2675\n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\nimport json\n\n# Load the arguments data (assuming it has columns 'id', 'premise')\narguments_df = pd.read_csv('/kaggle/input/projectvalues/Project_Data/arguments-validation.tsv', delimiter='\\t')\n\n# Load the value labels data (assuming it has columns 'id', 'value_label')\nlabels_df = pd.read_csv('/kaggle/input/projectvalues/Project_Data/labels-validation.tsv', delimiter='\\t')\n\n# Load the value descriptions from a JSON file\nwith open('/kaggle/input/value-categories/value-categories (1).json', 'r') as file:\n    value_descriptions = json.load(file)\n    \nlabels_long_df = labels_df.melt(id_vars='Argument ID', var_name='value_category', value_name='label')\nlabels_long_df = labels_long_df[labels_long_df['value_category'] != 'Universalism: objectivity']\nlabels_long_df['value_description'] = labels_long_df['value_category'].apply(lambda x: value_descriptions[x.lower().replace(\": \",\"-\")]['personal-motivation'])\ncombined_df_val = pd.merge(arguments_df, labels_long_df, left_on='Argument ID', right_on='Argument ID')\ncombined_df_val['Argument'] = combined_df_val.apply(\n    lambda row: f\"{row['Stance']} {row['Conclusion']} by saying {row['Premise']}\",\n    axis=1\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_inputs = tokenizer(list(combined_df_val['Argument']), list(combined_df_val['value_description']), padding=True, truncation=True, return_tensors=\"pt\")\ntest_input_ids = test_inputs['input_ids'].to(device)\ntest_attention_mask = test_inputs['attention_mask'].to(device)\ntest_labels = torch.tensor(combined_df_val['label'].values).to(device)\n\ntest_dataset = TensorDataset(test_input_ids, test_attention_mask, test_labels)\ntest_dataloader = DataLoader(test_dataset, batch_size=16)\n\n# Function to evaluate the model on the test set\ndef evaluate_model(model, dataloader):\n    model.eval()  # Set the model to evaluation mode\n    predictions, true_labels = [], []\n\n    with torch.no_grad():\n        for batch in dataloader:\n            b_input_ids, b_attention_mask, b_labels = [b.to(device) for b in batch]\n\n            outputs = model(b_input_ids, attention_mask=b_attention_mask)\n            logits = outputs.logits\n\n            logits = logits.detach().cpu().numpy()\n            label_ids = b_labels.to('cpu').numpy()\n\n            batch_predictions = np.argmax(logits, axis=1)\n            predictions.extend(batch_predictions)\n            true_labels.extend(label_ids)\n\n    return predictions, true_labels\n\n# Evaluate the model\n# predictions, true_labels = evaluate_model(model, test_dataloader)\n\n# # Calculate accuracy and F1 score\n# accuracy = accuracy_score(true_labels, predictions)\n# f1 = f1_score(true_labels, predictions, average='binary')\n\n# print(f\"Test Accuracy: {accuracy:.4f}\")\n# print(f\"Test F1 Score: {f1:.4f}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import Dataset,DataLoader\nprint(1)\nclass CategoryDataset(Dataset):\n    def __init__(self, dataframe, tokenizer, max_len=512):\n        self.tokenizer = tokenizer\n        self.data = dataframe\n        self.max_len = max_len\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        item = self.data.iloc[idx]\n        premise = item['Argument']\n        description = item['value_description']\n        label = item['label']\n        category = item['value_category']\n        \n        # Tokenize the text pair\n        encoding = self.tokenizer(premise, description, add_special_tokens=True, \n                                  max_length=self.max_len, padding='max_length', \n                                  truncation=True, return_tensors=\"pt\")\n        \n        input_ids = encoding['input_ids'].squeeze(0)  # Remove the batch dimension\n        attention_mask = encoding['attention_mask'].squeeze(0)\n        \n        return input_ids, attention_mask, torch.tensor(label, dtype=torch.float), category\n    \ndataset = CategoryDataset(combined_df_val,tokenizer)\ndataloader = DataLoader(dataset, batch_size=16, shuffle=False)\n\ndef evaluate_model(model, dataloader):\n    model.eval()  # Set the model to evaluation mode\n    all_logits = []\n    all_labels = []\n    all_categories = []\n\n    with torch.no_grad():\n        for input_ids, attention_mask, labels, categories in dataloader:\n            input_ids = input_ids.to(device)\n            attention_mask = attention_mask.to(device)\n            outputs = model(input_ids, attention_mask=attention_mask)\n            logits = outputs.logits  # Using sigmoid for binary classification\n            logits = logits.cpu().numpy()\n            labels = labels.cpu().numpy()\n            all_logits.extend(np.argmax(logits, axis=1))\n            all_labels.extend(labels)\n            all_categories.extend(categories)\n\n\n    # Convert lists to numpy arrays\n    all_logits = np.array(all_logits)\n    all_labels = np.array(all_labels)\n    all_categories = np.array(all_categories)\n\n    return all_logits, all_labels, all_categories\n\n# Get the predictions, true labels, and categories from the evaluation\nlogits, labels, categories = evaluate_model(model, dataloader)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f1_scores = {}\nunique_categories = np.unique(categories)\n\nfor category in unique_categories:\n    category_mask = (categories == category)\n    cat_labels = labels[category_mask]\n    cat_logits = logits[category_mask]\n#     print(cat_labels.shape)\n#     print(cat_logits.shape)\n    # Ensure that the shape of cat_labels and cat_logits is one-dimensional\n    cat_labels = np.squeeze(cat_labels)\n    cat_logits = np.squeeze(cat_logits)\n\n    # Calculate F1 score for the category\n    f1 = f1_score(cat_labels, cat_logits, average='binary')\n    f1_scores[category] = f1\n\n# Print F1 scores for each category\navg_score=0\nfor category, score in f1_scores.items():\n    print(f\"F1 Score for {category}: {score:.4f}\")\n    avg_score+=score\navg_score=avg_score/19\nprint(avg_score)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport json\n\n# Load the arguments data (assuming it has columns 'id', 'premise')\narguments_df = pd.read_csv('/kaggle/input/projectvalues/Project_Data/arguments-test.tsv', delimiter='\\t')\n\n# Load the value labels data (assuming it has columns 'id', 'value_label')\nlabels_df = pd.read_csv('/kaggle/input/projectvalues/Project_Data/labels-test.tsv', delimiter='\\t')\n\n# Load the value descriptions from a JSON file\nwith open('/kaggle/input/value-categories/value-categories (1).json', 'r') as file:\n    value_descriptions = json.load(file)\n    \nlabels_long_df = labels_df.melt(id_vars='Argument ID', var_name='value_category', value_name='label')\nlabels_long_df = labels_long_df[labels_long_df['value_category'] != 'Universalism: objectivity']\nlabels_long_df['value_description'] = labels_long_df['value_category'].apply(lambda x: value_descriptions[x.lower().replace(\": \",\"-\")]['personal-motivation'])\ncombined_df_test = pd.merge(arguments_df, labels_long_df, left_on='Argument ID', right_on='Argument ID')\ncombined_df_test['Argument'] = combined_df_test.apply(\n    lambda row: f\"{row['Stance']} {row['Conclusion']} by saying {row['Premise']}\",\n    axis=1\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_inputs = tokenizer(list(combined_df_test['Argument']), list(combined_df_test['value_description']), padding=True, truncation=True, return_tensors=\"pt\")\ntest_input_ids = test_inputs['input_ids'].to(device)\ntest_attention_mask = test_inputs['attention_mask'].to(device)\ntest_labels = torch.tensor(combined_df_test['label'].values).to(device)\n\ntest_dataset = TensorDataset(test_input_ids, test_attention_mask, test_labels)\ntest_dataloader = DataLoader(test_dataset, batch_size=16)\n\n# Function to evaluate the model on the test set\ndef evaluate_model(model, dataloader):\n    model.eval()  # Set the model to evaluation mode\n    predictions, true_labels = [], []\n\n    with torch.no_grad():\n        for batch in dataloader:\n            b_input_ids, b_attention_mask, b_labels = [b.to(device) for b in batch]\n\n            outputs = model(b_input_ids, attention_mask=b_attention_mask)\n            logits = outputs.logits\n\n            logits = logits.detach().cpu().numpy()\n            label_ids = b_labels.to('cpu').numpy()\n\n            batch_predictions = np.argmax(logits, axis=1)\n            predictions.extend(batch_predictions)\n            true_labels.extend(label_ids)\n\n    return predictions, true_labels\n\n# Evaluate the model\n# predictions, true_labels = evaluate_model(model, test_dataloader)\n\n# # Calculate accuracy and F1 score\n# accuracy = accuracy_score(true_labels, predictions)\n# f1 = f1_score(true_labels, predictions, average='binary')\n\n# print(f\"Test Accuracy: {accuracy:.4f}\")\n# print(f\"Test F1 Score: {f1:.4f}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import Dataset,DataLoader\n\nclass CategoryDataset(Dataset):\n    def __init__(self, dataframe, tokenizer, max_len=512):\n        self.tokenizer = tokenizer\n        self.data = dataframe\n        self.max_len = max_len\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        item = self.data.iloc[idx]\n        premise = item['Argument']\n        description = item['value_description']\n        label = item['label']\n        category = item['value_category']\n        \n        # Tokenize the text pair\n        encoding = self.tokenizer(premise, description, add_special_tokens=True, \n                                  max_length=self.max_len, padding='max_length', \n                                  truncation=True, return_tensors=\"pt\")\n        \n        input_ids = encoding['input_ids'].squeeze(0)  # Remove the batch dimension\n        attention_mask = encoding['attention_mask'].squeeze(0)\n        \n        return input_ids, attention_mask, torch.tensor(label, dtype=torch.float), category\n    \ndataset = CategoryDataset(combined_df_test,tokenizer)\ndataloader = DataLoader(dataset, batch_size=16, shuffle=False)\n\ndef evaluate_model(model, dataloader):\n    model.eval()  # Set the model to evaluation mode\n    all_logits = []\n    all_labels = []\n    all_categories = []\n\n    with torch.no_grad():\n        for input_ids, attention_mask, labels, categories in dataloader:\n            input_ids = input_ids.to(device)\n            attention_mask = attention_mask.to(device)\n            outputs = model(input_ids, attention_mask=attention_mask)\n            logits = outputs.logits  # Using sigmoid for binary classification\n            logits = logits.cpu().numpy()\n            labels = labels.cpu().numpy()\n            all_logits.extend(np.argmax(logits, axis=1))\n            all_labels.extend(labels)\n            all_categories.extend(categories)\n\n\n    # Convert lists to numpy arrays\n    all_logits = np.array(all_logits)\n    all_labels = np.array(all_labels)\n    all_categories = np.array(all_categories)\n\n    return all_logits, all_labels, all_categories\n\n# Get the predictions, true labels, and categories from the evaluation\nlogits, labels, categories = evaluate_model(model, dataloader)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f1_scores = {}\nunique_categories = np.unique(categories)\n\nfor category in unique_categories:\n    category_mask = (categories == category)\n    cat_labels = labels[category_mask]\n    cat_logits = logits[category_mask]\n#     print(cat_labels.shape)\n#     print(cat_logits.shape)\n    # Ensure that the shape of cat_labels and cat_logits is one-dimensional\n    cat_labels = np.squeeze(cat_labels)\n    cat_logits = np.squeeze(cat_logits)\n\n    # Calculate F1 score for the category\n    f1 = f1_score(cat_labels, cat_logits, average='binary')\n    f1_scores[category] = f1\n\n# Print F1 scores for each category\navg_score=0\nfor category, score in f1_scores.items():\n    print(f\"F1 Score for {category}: {score:.4f}\")\n    avg_score+=score\navg_score=avg_score/19\nprint(avg_score)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.save(model, 'model_entailment.pth')","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}