{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aayush/miniforge3/envs/dl_env/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import pickle\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from torch.utils.data import SequentialSampler, TensorDataset, RandomSampler\n",
    "from torch.cuda.amp import GradScaler\n",
    "from torch.cuda.amp import autocast\n",
    "from transformers import RobertaTokenizer, RobertaConfig, RobertaModel\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score, accuracy_score\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from datasets import load_dataset\n",
    "import time\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, AdamW\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_cats = json.load(open(\"/Users/aayush/Documents/IIITD/Assignments/NLP/Project/Final_Project/Project_Data/value-categories_aayush.json\"))\n",
    "tags = [\"training\", \"validation\"]\n",
    "data_dict = {}\n",
    "ratio_hard = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5393, 4) (5393, 21) (5393, 55) (5393, 78)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5393it [00:00, 14777.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1896, 4) (1896, 21) (1896, 55) (1896, 78)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1896it [00:00, 18297.11it/s]\n"
     ]
    }
   ],
   "source": [
    "for tag in tags:\n",
    "    data_dict[tag] = {}\n",
    "    arg_df = pd.read_csv(\"/Users/aayush/Documents/IIITD/Assignments/NLP/Project/Final_Project/Project_Data/arguments-\"+tag+\".tsv\", sep=\"\\t\")\n",
    "    label_df = pd.read_csv(\"/Users/aayush/Documents/IIITD/Assignments/NLP/Project/Final_Project/Project_Data/labels-\"+tag+\".tsv\", sep=\"\\t\")\n",
    "    level1_label_df = pd.read_csv(\"/Users/aayush/Documents/IIITD/Assignments/NLP/Project/Final_Project/Project_Data/level1-labels-\"+tag+\".tsv\", sep=\"\\t\")\n",
    "    merged_df = arg_df.merge(label_df, how=\"inner\", on =\"Argument ID\").merge(level1_label_df, \n",
    "                                                                             how=\"inner\", \n",
    "                                                                             on =\"Argument ID\").reset_index(drop=True)\n",
    "    print(arg_df.shape, label_df.shape, level1_label_df.shape, merged_df.shape)\n",
    "#     merged_df.head(2)\n",
    "\n",
    "    labels = [i for i in label_df.columns if i != 'Argument ID']\n",
    "    level_1 = [i for i in level1_label_df.columns if i != 'Argument ID']\n",
    "#     len(labels), len(level_1)\n",
    "\n",
    "    option_map = {}\n",
    "    for ix, row in merged_df.iterrows():\n",
    "        options = {}\n",
    "        used = []\n",
    "        for l in labels:\n",
    "            tmp = {}\n",
    "            if row[l] == 1:\n",
    "                for l1 in val_cats[l].keys():\n",
    "                    if row[l1] == 1:\n",
    "                        tmp[l1] = val_cats[l][l1]\n",
    "                        used.extend([l, l1])\n",
    "                options[l] = tmp\n",
    "        all_tagged = set([c for c in labels + level_1 if row[c] == 1])\n",
    "        assert len(all_tagged.difference(set(used))) == 0\n",
    "        option_map[row[\"Argument ID\"]] = options\n",
    "#     len(option_map)\n",
    "\n",
    "    for ix, row in tqdm(merged_df.iterrows()):\n",
    "        dct = {\"id\": row[\"Argument ID\"], \"stance\": row[\"Stance\"], \"premise\": row[\"Premise\"], \n",
    "               \"conclusion\": row[\"Conclusion\"], \"labels\": list(option_map[row[\"Argument ID\"]].keys())}\n",
    "        stance = \" against. \" if dct[\"stance\"] == \"against\" else \" in favor of. \"\n",
    "        dct[\"sent\"] = dct[\"premise\"] + stance + dct[\"conclusion\"]\n",
    "        dct[\"opts\"] = list(set([k2 + \" by \" + i for k, v in option_map[dct[\"id\"]].items() for k2, v2 in v.items() for i in v2]))\n",
    "\n",
    "        na_options_hard, na_options_easy = [], []\n",
    "        for k, v in option_map[dct[\"id\"]].items():\n",
    "            l1_present = set(v.keys())\n",
    "            l1_all = set(val_cats[k].keys())\n",
    "            assert len(l1_all) >= len(l1_present)\n",
    "            l1_not_present = l1_all.difference(l1_present)\n",
    "            na_options_hard.extend([i + \" by \" + j for i in list(l1_not_present) for j in val_cats[k][i]])\n",
    "\n",
    "        na_options_easy = [k + \" by \" + j for l in set(labels).difference(set(dct[\"labels\"])) \n",
    "                           for k, v in val_cats[l].items() \n",
    "                           for j in v]\n",
    "        random.shuffle(na_options_hard)\n",
    "        random.shuffle(na_options_easy)\n",
    "\n",
    "        hard_opts = na_options_hard[:int(len(dct[\"opts\"])*ratio_hard)]\n",
    "        easy_opts = na_options_easy[:(len(dct[\"opts\"]) - len(hard_opts))]\n",
    "        assert len(hard_opts) + len(easy_opts) == len(dct[\"opts\"])\n",
    "        dct[\"adverse_hard_opts\"], dct[\"adverse_easy_opts\"] = hard_opts, easy_opts\n",
    "        data_dict[tag][row[\"Argument ID\"]] = dct\n",
    "#     break\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input IDs Shape: torch.Size([2, 512])\n",
      "Labels Shape: torch.Size([2, 512])\n"
     ]
    }
   ],
   "source": [
    "data = data_dict['training']\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "\n",
    "class ArgumentDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_length):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.input_texts = []\n",
    "        self.target_texts = []\n",
    "\n",
    "        for item in data.values():\n",
    "            # id = list(item.keys())[0]\n",
    "            id = item['id']\n",
    "            premise = item['premise']\n",
    "            stance = \"against\" if item['stance'] == \"against\" else \"in favor of\"\n",
    "            labels = ', '.join(item['labels'])\n",
    "            conclusion = item['conclusion']\n",
    "            input_text = f\"Premise: {premise} Stance: {stance} Labels: {labels} Conclusion:\"\n",
    "            target_text = conclusion\n",
    "            \n",
    "            self.input_texts.append(input_text)\n",
    "            self.target_texts.append(target_text)\n",
    "\n",
    "        # Ensure to handle padding here correctly\n",
    "        self.inputs = tokenizer(self.input_texts, padding='max_length', truncation=True, max_length=max_length, return_tensors=\"pt\")\n",
    "        self.targets = tokenizer(self.target_texts, padding='max_length', truncation=True, max_length=max_length, return_tensors=\"pt\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_ids = self.inputs['input_ids'][idx]\n",
    "        attention_mask = self.inputs['attention_mask'][idx]\n",
    "        target_ids = self.targets['input_ids'][idx]\n",
    "        # Set padding parts in target_ids to -100 so they are not considered in loss calculation\n",
    "        target_ids[target_ids == tokenizer.pad_token_id] = -100\n",
    "        return input_ids, attention_mask, target_ids\n",
    "\n",
    "# Initialize Dataset and DataLoader\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "dataset = ArgumentDataset(data, tokenizer, max_length=512)\n",
    "loader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
    "\n",
    "# Check dimensions before training\n",
    "for input_ids, attention_mask, labels in loader:\n",
    "    print(f'Input IDs Shape: {input_ids.shape}')\n",
    "    print(f'Labels Shape: {labels.shape}')\n",
    "    break  # Break after the first batch to check\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id\n"
     ]
    }
   ],
   "source": [
    "for item in data.values():\n",
    "    id = list(item.keys())[0]\n",
    "    print(id)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in data.values():\n",
    "#     print(i['A01002']['premise'])\n",
    "#     break\n",
    "# print(len(data.values()))\n",
    "for item in data.values():\n",
    "    # id = list(item.keys())[0]\n",
    "    id = item['id']\n",
    "    premise = item['premise']\n",
    "    # print(premise)\n",
    "    stance = \"against\" if item['stance'] == \"against\" else \"in favor of\"\n",
    "    labels = ', '.join(item['labels'])\n",
    "    conclusion = item['conclusion']\n",
    "    input_text = f\"Premise: {premise} Stance: {stance} Labels: {labels} Conclusion:\"\n",
    "    target_text = conclusion\n",
    "    \n",
    "    # self.input_texts.append(input_text)\n",
    "    # self.target_texts.append(target_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aayush/miniforge3/envs/dl_env/lib/python3.11/site-packages/transformers/optimization.py:457: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/2697\n",
      "100/2697\n",
      "200/2697\n",
      "300/2697\n",
      "400/2697\n",
      "500/2697\n",
      "600/2697\n",
      "700/2697\n",
      "800/2697\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\")\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2').to(device)\n",
    "model.train()\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "num_epochs = 5\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (input_ids, attention_mask, labels) in enumerate(loader):\n",
    "        input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if(i%100==0):\n",
    "            print(f\"{i}/{len(loader)}\")\n",
    "    ### Print the loss after each epoch\n",
    "    print(f'Epoch {epoch+1}/{num_epochs} -> Loss: {loss.item()}')\n",
    "\n",
    "# Save the fine-tuned model\n",
    "model.save_pretrained('/Users/aayush/Documents/IIITD/Assignments/NLP/Project/Final_Project/Project_Data/conclusion_generation.pth')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "prompt = \"Premise: [Your premise] Stance: [in favor of/against] Labels: [Your labels] Conclusion:\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "generated_ids = model.generate(inputs['input_ids'], attention_mask=inputs['attention_mask'], max_length=100)\n",
    "conclusion = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "print(conclusion)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Assuming the class ArgumentDataset and model training code have been defined and executed as previously discussed\n",
    "\n",
    "# Function to generate text from the model\n",
    "def generate_text(model, tokenizer, device, loader):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    actuals = []\n",
    "    with torch.no_grad():\n",
    "        for input_ids, attention_mask, labels in tqdm(loader, desc=\"Generating text\"):\n",
    "            input_ids = input_ids.to(device)\n",
    "            attention_mask = attention_mask.to(device)\n",
    "            \n",
    "            # Generate predicted token ids\n",
    "            outputs = model.generate(input_ids, attention_mask=attention_mask, max_length=100)\n",
    "            pred_text = [tokenizer.decode(output, skip_special_tokens=True) for output in outputs]\n",
    "            actual_text = [tokenizer.decode(label[label != -100], skip_special_tokens=True) for label in labels.cpu()]\n",
    "            \n",
    "            predictions.extend(pred_text)\n",
    "            actuals.extend([[actual.split()] for actual in actual_text])  # BLEU expects a list of tokens for the references\n",
    "\n",
    "    return predictions, actuals\n",
    "\n",
    "# Load validation data\n",
    "val_data = data_dict['validation']  # Assuming data_dict contains the validation data\n",
    "val_dataset = ArgumentDataset(val_data, tokenizer, max_length=512)\n",
    "val_loader = DataLoader(val_dataset, batch_size=2, shuffle=False)\n",
    "\n",
    "# Generate predictions and actuals\n",
    "predictions, actuals = generate_text(model, tokenizer, device, val_loader)\n",
    "\n",
    "# Calculate BLEU score\n",
    "bleu_score = corpus_bleu(actuals, [pred.split() for pred in predictions])\n",
    "print(f\"BLEU Score on the Validation Set: {bleu_score:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
