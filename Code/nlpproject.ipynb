{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8194132,"sourceType":"datasetVersion","datasetId":4853226},{"sourceId":8194151,"sourceType":"datasetVersion","datasetId":4853242},{"sourceId":8206918,"sourceType":"datasetVersion","datasetId":4862918},{"sourceId":8206961,"sourceType":"datasetVersion","datasetId":4862955}],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-04-23T20:04:23.166523Z","iopub.execute_input":"2024-04-23T20:04:23.166918Z","iopub.status.idle":"2024-04-23T20:04:23.184114Z","shell.execute_reply.started":"2024-04-23T20:04:23.166891Z","shell.execute_reply":"2024-04-23T20:04:23.183260Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"/kaggle/input/valuecategories/value-categories.json\n/kaggle/input/projectdata/Project_Data/labels-test.tsv\n/kaggle/input/projectdata/Project_Data/arguments-validation.tsv\n/kaggle/input/projectdata/Project_Data/arguments-training.tsv\n/kaggle/input/projectdata/Project_Data/README.md\n/kaggle/input/projectdata/Project_Data/arguments-test.tsv\n/kaggle/input/projectdata/Project_Data/labels-validation.tsv\n/kaggle/input/projectdata/Project_Data/labels-training.tsv\n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\nimport json\n\n# Load the arguments data (assuming it has columns 'id', 'premise')\narguments_df = pd.read_csv('/kaggle/input/projectdata/Project_Data/arguments-training.tsv', delimiter='\\t')\n\n# Load the value labels data (assuming it has columns 'id', 'value_label')\nlabels_df = pd.read_csv('/kaggle/input/projectdata/Project_Data/labels-training.tsv', delimiter='\\t')\n\n# Load the value descriptions from a JSON file\nwith open('/kaggle/input/valuecategories/value-categories.json', 'r') as file:\n    value_descriptions = json.load(file)\n    \nlabels_long_df = labels_df.melt(id_vars='Argument ID', var_name='value_category', value_name='label')\nlabels_long_df = labels_long_df[labels_long_df['value_category'] != 'Universalism: objectivity']\nlabels_long_df['value_description'] = labels_long_df['value_category'].apply(lambda x: value_descriptions[x.lower().replace(\": \",\"-\")]['personal-motivation'])\ncombined_df = pd.merge(arguments_df, labels_long_df, left_on='Argument ID', right_on='Argument ID')\ncombined_df['Argument'] = combined_df.apply(\n    lambda row: f\"{row['Stance']} {row['Conclusion']} by saying {row['Premise']}\",\n    axis=1\n)","metadata":{"execution":{"iopub.status.busy":"2024-04-23T20:04:23.185862Z","iopub.execute_input":"2024-04-23T20:04:23.186240Z","iopub.status.idle":"2024-04-23T20:04:25.296615Z","shell.execute_reply.started":"2024-04-23T20:04:23.186208Z","shell.execute_reply":"2024-04-23T20:04:25.295814Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"final_df=combined_df\ndf_majority = final_df[final_df.label == 0]\ndf_minority = final_df[final_df.label == 1]\n\n# Determine the number of instances you want to keep from the majority class\n# For example, you might want to have a 1:1 ratio\nnumber_of_instances = len(df_minority)\n\n# Downsample the majority class\ndf_majority_downsampled = df_majority.sample(n=number_of_instances)\n\n# Combine the downsampled majority class with the minority class to get a balanced dataset\nbalanced_df = pd.concat([df_majority_downsampled, df_minority])\n\n# Shuffle the dataset to mix the two classes well\nbalanced_df = balanced_df.sample(frac=1).reset_index(drop=True)","metadata":{"execution":{"iopub.status.busy":"2024-04-23T20:04:25.297817Z","iopub.execute_input":"2024-04-23T20:04:25.298294Z","iopub.status.idle":"2024-04-23T20:04:25.349656Z","shell.execute_reply.started":"2024-04-23T20:04:25.298256Z","shell.execute_reply":"2024-04-23T20:04:25.348672Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"balanced_df","metadata":{"execution":{"iopub.status.busy":"2024-04-23T20:04:25.352415Z","iopub.execute_input":"2024-04-23T20:04:25.352846Z","iopub.status.idle":"2024-04-23T20:04:25.367314Z","shell.execute_reply.started":"2024-04-23T20:04:25.352814Z","shell.execute_reply":"2024-04-23T20:04:25.366458Z"},"trusted":true},"execution_count":20,"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"      Argument ID                                         Conclusion  \\\n0          A22325               We should adopt a multi-party system   \n1          A07038        We should abandon the use of school uniform   \n2          A09011           We should abolish the three-strikes laws   \n3          A18105                We should adopt an austerity regime   \n4          A18215                     We should ban targeted killing   \n...           ...                                                ...   \n34627      A24033                     We should subsidize journalism   \n34628      E02069  We need a common migration and asylum policy, ...   \n34629      A25324  We should abolish the right to keep and bear arms   \n34630      A26071           We should abolish the three-strikes laws   \n34631      A21275                      We should subsidize Wikipedia   \n\n            Stance                                            Premise  \\\n0          against  this would result in inaccurate representation...   \n1      in favor of  Allowing students to make their own choices re...   \n2      in favor of  Three strikes laws often punish those who have...   \n3      in favor of  we should adopt an austerity regime so that fu...   \n4          against  we should  not ban targeted killing, it preven...   \n...            ...                                                ...   \n34627  in favor of  journalism needs diversitiy desperately becaus...   \n34628      against  We should not bring the problem to Europe, but...   \n34629      against  if the right to keep and bear arms was abolish...   \n34630      against  we should not abolish the three-strikes laws b...   \n34631  in favor of  wikipedia is useful in helping people of all a...   \n\n                  value_category  label  \\\n0                      Tradition      1   \n1                    Stimulation      1   \n2      Conformity: interpersonal      0   \n3                    Achievement      1   \n4         Self-direction: action      0   \n...                          ...    ...   \n34627    Self-direction: thought      1   \n34628           Power: resources      0   \n34629                  Tradition      0   \n34630                   Humility      0   \n34631                   Hedonism      0   \n\n                                       value_description  \\\n0      Maintain traditional beliefs and values, follo...   \n1      Always looking for something new to do, doing ...   \n2      Avoid upsetting or annoying others, being tact...   \n3      Being ambitious, successful and being admired ...   \n4      It is important to make own decisions about li...   \n...                                                  ...   \n34627  It is important to be creative, forming own op...   \n34628  Having lots of money for the power it brings, ...   \n34629  Maintain traditional beliefs and values, follo...   \n34630  Try not to draw attention, be humble and satis...   \n34631  Having a good time, enjoying life’s pleasures ...   \n\n                                                Argument  \n0      against We should adopt a multi-party system b...  \n1      in favor of We should abandon the use of schoo...  \n2      in favor of We should abolish the three-strike...  \n3      in favor of We should adopt an austerity regim...  \n4      against We should ban targeted killing by sayi...  \n...                                                  ...  \n34627  in favor of We should subsidize journalism by ...  \n34628  against We need a common migration and asylum ...  \n34629  against We should abolish the right to keep an...  \n34630  against We should abolish the three-strikes la...  \n34631  in favor of We should subsidize Wikipedia by s...  \n\n[34632 rows x 8 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Argument ID</th>\n      <th>Conclusion</th>\n      <th>Stance</th>\n      <th>Premise</th>\n      <th>value_category</th>\n      <th>label</th>\n      <th>value_description</th>\n      <th>Argument</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>A22325</td>\n      <td>We should adopt a multi-party system</td>\n      <td>against</td>\n      <td>this would result in inaccurate representation...</td>\n      <td>Tradition</td>\n      <td>1</td>\n      <td>Maintain traditional beliefs and values, follo...</td>\n      <td>against We should adopt a multi-party system b...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>A07038</td>\n      <td>We should abandon the use of school uniform</td>\n      <td>in favor of</td>\n      <td>Allowing students to make their own choices re...</td>\n      <td>Stimulation</td>\n      <td>1</td>\n      <td>Always looking for something new to do, doing ...</td>\n      <td>in favor of We should abandon the use of schoo...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>A09011</td>\n      <td>We should abolish the three-strikes laws</td>\n      <td>in favor of</td>\n      <td>Three strikes laws often punish those who have...</td>\n      <td>Conformity: interpersonal</td>\n      <td>0</td>\n      <td>Avoid upsetting or annoying others, being tact...</td>\n      <td>in favor of We should abolish the three-strike...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>A18105</td>\n      <td>We should adopt an austerity regime</td>\n      <td>in favor of</td>\n      <td>we should adopt an austerity regime so that fu...</td>\n      <td>Achievement</td>\n      <td>1</td>\n      <td>Being ambitious, successful and being admired ...</td>\n      <td>in favor of We should adopt an austerity regim...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>A18215</td>\n      <td>We should ban targeted killing</td>\n      <td>against</td>\n      <td>we should  not ban targeted killing, it preven...</td>\n      <td>Self-direction: action</td>\n      <td>0</td>\n      <td>It is important to make own decisions about li...</td>\n      <td>against We should ban targeted killing by sayi...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>34627</th>\n      <td>A24033</td>\n      <td>We should subsidize journalism</td>\n      <td>in favor of</td>\n      <td>journalism needs diversitiy desperately becaus...</td>\n      <td>Self-direction: thought</td>\n      <td>1</td>\n      <td>It is important to be creative, forming own op...</td>\n      <td>in favor of We should subsidize journalism by ...</td>\n    </tr>\n    <tr>\n      <th>34628</th>\n      <td>E02069</td>\n      <td>We need a common migration and asylum policy, ...</td>\n      <td>against</td>\n      <td>We should not bring the problem to Europe, but...</td>\n      <td>Power: resources</td>\n      <td>0</td>\n      <td>Having lots of money for the power it brings, ...</td>\n      <td>against We need a common migration and asylum ...</td>\n    </tr>\n    <tr>\n      <th>34629</th>\n      <td>A25324</td>\n      <td>We should abolish the right to keep and bear arms</td>\n      <td>against</td>\n      <td>if the right to keep and bear arms was abolish...</td>\n      <td>Tradition</td>\n      <td>0</td>\n      <td>Maintain traditional beliefs and values, follo...</td>\n      <td>against We should abolish the right to keep an...</td>\n    </tr>\n    <tr>\n      <th>34630</th>\n      <td>A26071</td>\n      <td>We should abolish the three-strikes laws</td>\n      <td>against</td>\n      <td>we should not abolish the three-strikes laws b...</td>\n      <td>Humility</td>\n      <td>0</td>\n      <td>Try not to draw attention, be humble and satis...</td>\n      <td>against We should abolish the three-strikes la...</td>\n    </tr>\n    <tr>\n      <th>34631</th>\n      <td>A21275</td>\n      <td>We should subsidize Wikipedia</td>\n      <td>in favor of</td>\n      <td>wikipedia is useful in helping people of all a...</td>\n      <td>Hedonism</td>\n      <td>0</td>\n      <td>Having a good time, enjoying life’s pleasures ...</td>\n      <td>in favor of We should subsidize Wikipedia by s...</td>\n    </tr>\n  </tbody>\n</table>\n<p>34632 rows × 8 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"from torch.utils.data import DataLoader, TensorDataset, random_split\nfrom transformers import AdamW, get_linear_schedule_with_warmup\nfrom torch.nn.functional import cross_entropy\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer\nimport torch\nfrom sklearn.metrics import accuracy_score,f1_score\nmodel_name = 'pepa/roberta-base-snli'\nconfig = AutoModelForSequenceClassification.from_pretrained(model_name, \n                                                             return_dict=True,\n                                                             output_hidden_states=False,\n                                                             hidden_dropout_prob=0.3,  # Set dropout probability for hidden layers\n                                                             attention_probs_dropout_prob=0.3)  # Set dropout probability for attention layers\n\n\n# Verify that we are only optimizing biases\n\nmodel = AutoModelForSequenceClassification.from_pretrained(model_name, config=config)\nclassifier_parameters = []\nbias_parameters = []\nfor name, param in model.named_parameters():\n    if 'classifier' in name or 'bias' in name:\n        classifier_parameters.append(param)\n    else:\n        param.requires_grad = False\nprint(f\"Total trainable parameters: {len(classifier_parameters)}\")\ntokenizer = AutoTokenizer.from_pretrained(model_name)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel=model.to(device)\ninputs = tokenizer(list(balanced_df['Argument']), list(balanced_df['value_description']), padding=True, truncation=True, return_tensors=\"pt\")\ninput_ids = inputs['input_ids']\nattention_mask = inputs['attention_mask']\nlabels = torch.tensor(balanced_df['label'].values)","metadata":{"execution":{"iopub.status.busy":"2024-04-23T20:04:25.368548Z","iopub.execute_input":"2024-04-23T20:04:25.368835Z","iopub.status.idle":"2024-04-23T20:04:41.182535Z","shell.execute_reply.started":"2024-04-23T20:04:25.368813Z","shell.execute_reply":"2024-04-23T20:04:41.181672Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"Total trainable parameters: 101\n","output_type":"stream"}]},{"cell_type":"code","source":"train_dataset = TensorDataset(input_ids, attention_mask, labels)\ntrain_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=8)\noptimizer = AdamW(classifier_parameters, lr=2e-5)\nepochs=3\ntotal_steps = len(train_dataloader) * epochs\nscheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\nimport numpy as np\n# Define a training loop\nfor epoch in range(epochs):\n    model.train()\n    total_loss = 0\n    predictions = []\n    true_labels = []\n    steps=1\n    i=0\n    for batch in train_dataloader:\n        b_input_ids, b_attention_mask, b_labels = batch\n        b_input_ids = b_input_ids.to(device)\n        b_attention_mask = b_attention_mask.to(device)\n        b_labels = b_labels.to(device)\n        \n        # Clear any previously calculated gradients\n        optimizer.zero_grad()\n        \n        # Perform a forward pass. This will return logits.\n        outputs = model(b_input_ids, attention_mask=b_attention_mask, labels=b_labels)\n        \n        # Calculate loss using the outputs and the labels\n        loss = outputs[0]\n        total_loss += loss.item()\n        logits = outputs.logits.detach().cpu().numpy()\n        label_ids = b_labels.to('cpu').numpy()\n        predictions.extend(np.argmax(logits, axis=1).flatten())\n        true_labels.extend(label_ids.flatten())\n        \n        # Perform a backward pass to calculate gradients\n        loss.backward()\n        \n        # Update parameters and take a step using the computed gradient\n        optimizer.step()\n        \n        # Update the learning rate\n        scheduler.step()\n        if steps % 200 == 0:\n            interim_f1 = f1_score(true_labels, predictions, average='macro')\n            print(f\"Epoch {epoch}, Step {steps}, Loss: {total_loss / steps:.4f}, Interim F1 Score: {interim_f1:.4f}\")\n            predictions = []  # Reset predictions\n            true_labels = []  # Reset true labels\n        steps+=1\n    \n    # Calculate the average loss over the training data\n    avg_train_loss = total_loss / len(train_dataloader)\n\n\nprint(\"Training complete\")","metadata":{"execution":{"iopub.status.busy":"2024-04-23T20:04:41.183904Z","iopub.execute_input":"2024-04-23T20:04:41.184379Z","iopub.status.idle":"2024-04-23T20:57:01.085972Z","shell.execute_reply.started":"2024-04-23T20:04:41.184345Z","shell.execute_reply":"2024-04-23T20:57:01.084982Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:457: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Epoch 0, Step 200, Loss: 0.9024, Interim F1 Score: 0.2858\nEpoch 0, Step 400, Loss: 0.7875, Interim F1 Score: 0.6120\nEpoch 0, Step 600, Loss: 0.7390, Interim F1 Score: 0.6499\nEpoch 0, Step 800, Loss: 0.7104, Interim F1 Score: 0.6531\nEpoch 0, Step 1000, Loss: 0.6941, Interim F1 Score: 0.6594\nEpoch 0, Step 1200, Loss: 0.6844, Interim F1 Score: 0.6548\nEpoch 0, Step 1400, Loss: 0.6751, Interim F1 Score: 0.6662\nEpoch 0, Step 1600, Loss: 0.6674, Interim F1 Score: 0.6598\nEpoch 0, Step 1800, Loss: 0.6612, Interim F1 Score: 0.6794\nEpoch 0, Step 2000, Loss: 0.6570, Interim F1 Score: 0.6623\nEpoch 0, Step 2200, Loss: 0.6536, Interim F1 Score: 0.6813\nEpoch 0, Step 2400, Loss: 0.6503, Interim F1 Score: 0.6680\nEpoch 0, Step 2600, Loss: 0.6483, Interim F1 Score: 0.6431\nEpoch 0, Step 2800, Loss: 0.6443, Interim F1 Score: 0.6854\nEpoch 0, Step 3000, Loss: 0.6412, Interim F1 Score: 0.6818\nEpoch 0, Step 3200, Loss: 0.6378, Interim F1 Score: 0.6979\nEpoch 0, Step 3400, Loss: 0.6355, Interim F1 Score: 0.6883\nEpoch 0, Step 3600, Loss: 0.6336, Interim F1 Score: 0.6843\nEpoch 0, Step 3800, Loss: 0.6312, Interim F1 Score: 0.6851\nEpoch 0, Step 4000, Loss: 0.6293, Interim F1 Score: 0.6888\nEpoch 0, Step 4200, Loss: 0.6280, Interim F1 Score: 0.6759\nEpoch 1, Step 200, Loss: 0.6008, Interim F1 Score: 0.6828\nEpoch 1, Step 400, Loss: 0.5903, Interim F1 Score: 0.7000\nEpoch 1, Step 600, Loss: 0.5907, Interim F1 Score: 0.6900\nEpoch 1, Step 800, Loss: 0.5932, Interim F1 Score: 0.6819\nEpoch 1, Step 1000, Loss: 0.5942, Interim F1 Score: 0.6805\nEpoch 1, Step 1200, Loss: 0.5934, Interim F1 Score: 0.6878\nEpoch 1, Step 1400, Loss: 0.5938, Interim F1 Score: 0.6685\nEpoch 1, Step 1600, Loss: 0.5920, Interim F1 Score: 0.6930\nEpoch 1, Step 1800, Loss: 0.5912, Interim F1 Score: 0.6842\nEpoch 1, Step 2000, Loss: 0.5902, Interim F1 Score: 0.6919\nEpoch 1, Step 2200, Loss: 0.5888, Interim F1 Score: 0.7156\nEpoch 1, Step 2400, Loss: 0.5893, Interim F1 Score: 0.6883\nEpoch 1, Step 2600, Loss: 0.5885, Interim F1 Score: 0.7044\nEpoch 1, Step 2800, Loss: 0.5888, Interim F1 Score: 0.6850\nEpoch 1, Step 3000, Loss: 0.5879, Interim F1 Score: 0.7033\nEpoch 1, Step 3200, Loss: 0.5878, Interim F1 Score: 0.6980\nEpoch 1, Step 3400, Loss: 0.5878, Interim F1 Score: 0.6865\nEpoch 1, Step 3600, Loss: 0.5882, Interim F1 Score: 0.6753\nEpoch 1, Step 3800, Loss: 0.5869, Interim F1 Score: 0.7036\nEpoch 1, Step 4000, Loss: 0.5864, Interim F1 Score: 0.7085\nEpoch 1, Step 4200, Loss: 0.5868, Interim F1 Score: 0.6907\nEpoch 2, Step 200, Loss: 0.5584, Interim F1 Score: 0.7111\nEpoch 2, Step 400, Loss: 0.5735, Interim F1 Score: 0.6822\nEpoch 2, Step 600, Loss: 0.5762, Interim F1 Score: 0.7042\nEpoch 2, Step 800, Loss: 0.5771, Interim F1 Score: 0.6816\nEpoch 2, Step 1000, Loss: 0.5750, Interim F1 Score: 0.7100\nEpoch 2, Step 1200, Loss: 0.5777, Interim F1 Score: 0.6802\nEpoch 2, Step 1400, Loss: 0.5790, Interim F1 Score: 0.6890\nEpoch 2, Step 1600, Loss: 0.5765, Interim F1 Score: 0.7144\nEpoch 2, Step 1800, Loss: 0.5773, Interim F1 Score: 0.7020\nEpoch 2, Step 2000, Loss: 0.5781, Interim F1 Score: 0.6894\nEpoch 2, Step 2200, Loss: 0.5777, Interim F1 Score: 0.6982\nEpoch 2, Step 2400, Loss: 0.5780, Interim F1 Score: 0.6937\nEpoch 2, Step 2600, Loss: 0.5780, Interim F1 Score: 0.6900\nEpoch 2, Step 2800, Loss: 0.5788, Interim F1 Score: 0.6837\nEpoch 2, Step 3000, Loss: 0.5794, Interim F1 Score: 0.6888\nEpoch 2, Step 3200, Loss: 0.5793, Interim F1 Score: 0.6963\nEpoch 2, Step 3400, Loss: 0.5793, Interim F1 Score: 0.6946\nEpoch 2, Step 3600, Loss: 0.5794, Interim F1 Score: 0.7091\nEpoch 2, Step 3800, Loss: 0.5791, Interim F1 Score: 0.6955\nEpoch 2, Step 4000, Loss: 0.5789, Interim F1 Score: 0.6969\nEpoch 2, Step 4200, Loss: 0.5787, Interim F1 Score: 0.6903\nTraining complete\n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\nimport json\n\n# Load the arguments data (assuming it has columns 'id', 'premise')\narguments_df = pd.read_csv('/kaggle/input/projectdata/Project_Data/arguments-validation.tsv', delimiter='\\t')\n\n# Load the value labels data (assuming it has columns 'id', 'value_label')\nlabels_df = pd.read_csv('/kaggle/input/projectdata/Project_Data/labels-validation.tsv', delimiter='\\t')\n\n# Load the value descriptions from a JSON file\nwith open('/kaggle/input/valuecategories/value-categories.json', 'r') as file:\n    value_descriptions = json.load(file)\n    \nlabels_long_df = labels_df.melt(id_vars='Argument ID', var_name='value_category', value_name='label')\nlabels_long_df = labels_long_df[labels_long_df['value_category'] != 'Universalism: objectivity']\nlabels_long_df['value_description'] = labels_long_df['value_category'].apply(lambda x: value_descriptions[x.lower().replace(\": \",\"-\")]['personal-motivation'])\ncombined_df_val = pd.merge(arguments_df, labels_long_df, left_on='Argument ID', right_on='Argument ID')\ncombined_df_val['Argument'] = combined_df_val.apply(\n    lambda row: f\"{row['Stance']} {row['Conclusion']} by saying {row['Premise']}\",\n    axis=1\n)","metadata":{"execution":{"iopub.status.busy":"2024-04-23T20:57:01.087343Z","iopub.execute_input":"2024-04-23T20:57:01.087720Z","iopub.status.idle":"2024-04-23T20:57:01.835457Z","shell.execute_reply.started":"2024-04-23T20:57:01.087686Z","shell.execute_reply":"2024-04-23T20:57:01.834653Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"test_inputs = tokenizer(list(combined_df_val['Argument']), list(combined_df_val['value_description']), padding=True, truncation=True, return_tensors=\"pt\")\ntest_input_ids = test_inputs['input_ids'].to(device)\ntest_attention_mask = test_inputs['attention_mask'].to(device)\ntest_labels = torch.tensor(combined_df_val['label'].values).to(device)\n\ntest_dataset = TensorDataset(test_input_ids, test_attention_mask, test_labels)\ntest_dataloader = DataLoader(test_dataset, batch_size=16)\n\n# Function to evaluate the model on the test set\ndef evaluate_model(model, dataloader):\n    model.eval()  # Set the model to evaluation mode\n    predictions, true_labels = [], []\n\n    with torch.no_grad():\n        for batch in dataloader:\n            b_input_ids, b_attention_mask, b_labels = [b.to(device) for b in batch]\n\n            outputs = model(b_input_ids, attention_mask=b_attention_mask)\n            logits = outputs.logits\n\n            logits = logits.detach().cpu().numpy()\n            label_ids = b_labels.to('cpu').numpy()\n\n            batch_predictions = np.argmax(logits, axis=1)\n            predictions.extend(batch_predictions)\n            true_labels.extend(label_ids)\n\n    return predictions, true_labels\n\n# Evaluate the model\n# predictions, true_labels = evaluate_model(model, test_dataloader)\n\n# # Calculate accuracy and F1 score\n# accuracy = accuracy_score(true_labels, predictions)\n# f1 = f1_score(true_labels, predictions, average='binary')\n\n# print(f\"Test Accuracy: {accuracy:.4f}\")\n# print(f\"Test F1 Score: {f1:.4f}\")","metadata":{"execution":{"iopub.status.busy":"2024-04-23T20:57:01.836537Z","iopub.execute_input":"2024-04-23T20:57:01.836820Z","iopub.status.idle":"2024-04-23T20:57:17.031956Z","shell.execute_reply.started":"2024-04-23T20:57:01.836796Z","shell.execute_reply":"2024-04-23T20:57:17.031155Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import Dataset,DataLoader\nprint(1)\nclass CategoryDataset(Dataset):\n    def __init__(self, dataframe, tokenizer, max_len=512):\n        self.tokenizer = tokenizer\n        self.data = dataframe\n        self.max_len = max_len\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        item = self.data.iloc[idx]\n        premise = item['Argument']\n        description = item['value_description']\n        label = item['label']\n        category = item['value_category']\n        \n        # Tokenize the text pair\n        encoding = self.tokenizer(premise, description, add_special_tokens=True, \n                                  max_length=self.max_len, padding='max_length', \n                                  truncation=True, return_tensors=\"pt\")\n        \n        input_ids = encoding['input_ids'].squeeze(0)  # Remove the batch dimension\n        attention_mask = encoding['attention_mask'].squeeze(0)\n        \n        return input_ids, attention_mask, torch.tensor(label, dtype=torch.float), category\n    \ndataset = CategoryDataset(combined_df_val,tokenizer)\ndataloader = DataLoader(dataset, batch_size=16, shuffle=False)\n\ndef evaluate_model(model, dataloader):\n    model.eval()  # Set the model to evaluation mode\n    all_logits = []\n    all_labels = []\n    all_categories = []\n\n    with torch.no_grad():\n        for input_ids, attention_mask, labels, categories in dataloader:\n            input_ids = input_ids.to(device)\n            attention_mask = attention_mask.to(device)\n            outputs = model(input_ids, attention_mask=attention_mask)\n            logits = outputs.logits  # Using sigmoid for binary classification\n            logits = logits.cpu().numpy()\n            labels = labels.cpu().numpy()\n            all_logits.extend(np.argmax(logits, axis=1))\n            all_labels.extend(labels)\n            all_categories.extend(categories)\n\n\n    # Convert lists to numpy arrays\n    all_logits = np.array(all_logits)\n    all_labels = np.array(all_labels)\n    all_categories = np.array(all_categories)\n\n    return all_logits, all_labels, all_categories\n\n# Get the predictions, true labels, and categories from the evaluation\nlogits, labels, categories = evaluate_model(model, dataloader)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-23T20:57:17.033278Z","iopub.execute_input":"2024-04-23T20:57:17.033624Z","iopub.status.idle":"2024-04-23T21:08:19.946460Z","shell.execute_reply.started":"2024-04-23T20:57:17.033591Z","shell.execute_reply":"2024-04-23T21:08:19.945623Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stdout","text":"1\n","output_type":"stream"}]},{"cell_type":"code","source":"f1_scores = {}\nunique_categories = np.unique(categories)\n\nfor category in unique_categories:\n    category_mask = (categories == category)\n    cat_labels = labels[category_mask]\n    cat_logits = logits[category_mask]\n#     print(cat_labels.shape)\n#     print(cat_logits.shape)\n    # Ensure that the shape of cat_labels and cat_logits is one-dimensional\n    cat_labels = np.squeeze(cat_labels)\n    cat_logits = np.squeeze(cat_logits)\n\n    # Calculate F1 score for the category\n    f1 = f1_score(cat_labels, cat_logits, average='macro')\n    f1_scores[category] = f1\n\n# Print F1 scores for each category\navg_score=0\nfor category, score in f1_scores.items():\n    print(f\"F1 Score for {category}: {score:.4f}\")\n    avg_score+=score\navg_score=avg_score/19\nprint(avg_score)","metadata":{"execution":{"iopub.status.busy":"2024-04-23T21:08:19.949938Z","iopub.execute_input":"2024-04-23T21:08:19.950383Z","iopub.status.idle":"2024-04-23T21:08:20.011537Z","shell.execute_reply.started":"2024-04-23T21:08:19.950355Z","shell.execute_reply":"2024-04-23T21:08:20.010563Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stdout","text":"F1 Score for Achievement: 0.5364\nF1 Score for Benevolence: caring: 0.5144\nF1 Score for Benevolence: dependability: 0.5046\nF1 Score for Conformity: interpersonal: 0.4894\nF1 Score for Conformity: rules: 0.5350\nF1 Score for Face: 0.5152\nF1 Score for Hedonism: 0.5954\nF1 Score for Humility: 0.5041\nF1 Score for Power: dominance: 0.5395\nF1 Score for Power: resources: 0.6682\nF1 Score for Security: personal: 0.2997\nF1 Score for Security: societal: 0.3534\nF1 Score for Self-direction: action: 0.5035\nF1 Score for Self-direction: thought: 0.5372\nF1 Score for Stimulation: 0.4953\nF1 Score for Tradition: 0.6512\nF1 Score for Universalism: concern: 0.2719\nF1 Score for Universalism: nature: 0.7447\nF1 Score for Universalism: tolerance: 0.5559\n0.5165775782438357\n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\nimport json\n\n# Load the arguments data (assuming it has columns 'id', 'premise')\narguments_df = pd.read_csv('/kaggle/input/projectdata/Project_Data/arguments-test.tsv', delimiter='\\t')\n\n# Load the value labels data (assuming it has columns 'id', 'value_label')\nlabels_df = pd.read_csv('/kaggle/input/projectdata/Project_Data/labels-test.tsv', delimiter='\\t')\n\n# Load the value descriptions from a JSON file\nwith open('/kaggle/input/valuecategories/value-categories.json', 'r') as file:\n    value_descriptions = json.load(file)\n    \nlabels_long_df = labels_df.melt(id_vars='Argument ID', var_name='value_category', value_name='label')\nlabels_long_df = labels_long_df[labels_long_df['value_category'] != 'Universalism: objectivity']\nlabels_long_df['value_description'] = labels_long_df['value_category'].apply(lambda x: value_descriptions[x.lower().replace(\": \",\"-\")]['personal-motivation'])\ncombined_df_test = pd.merge(arguments_df, labels_long_df, left_on='Argument ID', right_on='Argument ID')\ncombined_df_test['Argument'] = combined_df_test.apply(\n    lambda row: f\"{row['Stance']} {row['Conclusion']} by saying {row['Premise']}\",\n    axis=1\n)","metadata":{"execution":{"iopub.status.busy":"2024-04-23T21:08:20.012744Z","iopub.execute_input":"2024-04-23T21:08:20.013031Z","iopub.status.idle":"2024-04-23T21:08:20.645231Z","shell.execute_reply.started":"2024-04-23T21:08:20.013008Z","shell.execute_reply":"2024-04-23T21:08:20.644252Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"test_inputs = tokenizer(list(combined_df_test['Argument']), list(combined_df_test['value_description']), padding=True, truncation=True, return_tensors=\"pt\")\ntest_input_ids = test_inputs['input_ids'].to(device)\ntest_attention_mask = test_inputs['attention_mask'].to(device)\ntest_labels = torch.tensor(combined_df_test['label'].values).to(device)\n\ntest_dataset = TensorDataset(test_input_ids, test_attention_mask, test_labels)\ntest_dataloader = DataLoader(test_dataset, batch_size=16)\n\n# Function to evaluate the model on the test set\ndef evaluate_model(model, dataloader):\n    model.eval()  # Set the model to evaluation mode\n    predictions, true_labels = [], []\n\n    with torch.no_grad():\n        for batch in dataloader:\n            b_input_ids, b_attention_mask, b_labels = [b.to(device) for b in batch]\n\n            outputs = model(b_input_ids, attention_mask=b_attention_mask)\n            logits = outputs.logits\n\n            logits = logits.detach().cpu().numpy()\n            label_ids = b_labels.to('cpu').numpy()\n\n            batch_predictions = np.argmax(logits, axis=1)\n            predictions.extend(batch_predictions)\n            true_labels.extend(label_ids)\n\n    return predictions, true_labels\n\n# Evaluate the model\n# predictions, true_labels = evaluate_model(model, test_dataloader)\n\n# # Calculate accuracy and F1 score\n# accuracy = accuracy_score(true_labels, predictions)\n# f1 = f1_score(true_labels, predictions, average='binary')\n\n# print(f\"Test Accuracy: {accuracy:.4f}\")\n# print(f\"Test F1 Score: {f1:.4f}\")","metadata":{"execution":{"iopub.status.busy":"2024-04-23T21:08:20.646373Z","iopub.execute_input":"2024-04-23T21:08:20.646670Z","iopub.status.idle":"2024-04-23T21:08:34.544698Z","shell.execute_reply.started":"2024-04-23T21:08:20.646646Z","shell.execute_reply":"2024-04-23T21:08:34.543891Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import Dataset,DataLoader\n\nclass CategoryDataset(Dataset):\n    def __init__(self, dataframe, tokenizer, max_len=512):\n        self.tokenizer = tokenizer\n        self.data = dataframe\n        self.max_len = max_len\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        item = self.data.iloc[idx]\n        premise = item['Argument']\n        description = item['value_description']\n        label = item['label']\n        category = item['value_category']\n        \n        # Tokenize the text pair\n        encoding = self.tokenizer(premise, description, add_special_tokens=True, \n                                  max_length=self.max_len, padding='max_length', \n                                  truncation=True, return_tensors=\"pt\")\n        \n        input_ids = encoding['input_ids'].squeeze(0)  # Remove the batch dimension\n        attention_mask = encoding['attention_mask'].squeeze(0)\n        \n        return input_ids, attention_mask, torch.tensor(label, dtype=torch.float), category\n    \ndataset = CategoryDataset(combined_df_test,tokenizer)\ndataloader = DataLoader(dataset, batch_size=16, shuffle=False)\n\ndef evaluate_model(model, dataloader):\n    model.eval()  # Set the model to evaluation mode\n    all_logits = []\n    all_labels = []\n    all_categories = []\n\n    with torch.no_grad():\n        for input_ids, attention_mask, labels, categories in dataloader:\n            input_ids = input_ids.to(device)\n            attention_mask = attention_mask.to(device)\n            outputs = model(input_ids, attention_mask=attention_mask)\n            logits = outputs.logits  # Using sigmoid for binary classification\n            logits = logits.cpu().numpy()\n            labels = labels.cpu().numpy()\n            all_logits.extend(np.argmax(logits, axis=1))\n            all_labels.extend(labels)\n            all_categories.extend(categories)\n\n\n    # Convert lists to numpy arrays\n    all_logits = np.array(all_logits)\n    all_labels = np.array(all_labels)\n    all_categories = np.array(all_categories)\n\n    return all_logits, all_labels, all_categories\n\n# Get the predictions, true labels, and categories from the evaluation\nlogits, labels, categories = evaluate_model(model, dataloader)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-23T21:08:34.545967Z","iopub.execute_input":"2024-04-23T21:08:34.546345Z","iopub.status.idle":"2024-04-23T21:17:45.820264Z","shell.execute_reply.started":"2024-04-23T21:08:34.546314Z","shell.execute_reply":"2024-04-23T21:17:45.819254Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"f1_scores = {}\nunique_categories = np.unique(categories)\n\nfor category in unique_categories:\n    category_mask = (categories == category)\n    cat_labels = labels[category_mask]\n    cat_logits = logits[category_mask]\n#     print(cat_labels.shape)\n#     print(cat_logits.shape)\n    # Ensure that the shape of cat_labels and cat_logits is one-dimensional\n    cat_labels = np.squeeze(cat_labels)\n    cat_logits = np.squeeze(cat_logits)\n\n    # Calculate F1 score for the category\n    f1 = f1_score(cat_labels, cat_logits, average='macro')\n    f1_scores[category] = f1\n\n# Print F1 scores for each category\navg_score=0\nfor category, score in f1_scores.items():\n    print(f\"F1 Score for {category}: {score:.4f}\")\n    avg_score+=score\navg_score=avg_score/19\nprint(avg_score)","metadata":{"execution":{"iopub.status.busy":"2024-04-23T21:17:45.821663Z","iopub.execute_input":"2024-04-23T21:17:45.822329Z","iopub.status.idle":"2024-04-23T21:17:45.876461Z","shell.execute_reply.started":"2024-04-23T21:17:45.822295Z","shell.execute_reply":"2024-04-23T21:17:45.875592Z"},"trusted":true},"execution_count":30,"outputs":[{"name":"stdout","text":"F1 Score for Achievement: 0.4560\nF1 Score for Benevolence: caring: 0.4287\nF1 Score for Benevolence: dependability: 0.5323\nF1 Score for Conformity: interpersonal: 0.5203\nF1 Score for Conformity: rules: 0.5212\nF1 Score for Face: 0.5221\nF1 Score for Hedonism: 0.5853\nF1 Score for Humility: 0.5202\nF1 Score for Power: dominance: 0.5516\nF1 Score for Power: resources: 0.6412\nF1 Score for Security: personal: 0.2727\nF1 Score for Security: societal: 0.2583\nF1 Score for Self-direction: action: 0.4764\nF1 Score for Self-direction: thought: 0.5095\nF1 Score for Stimulation: 0.5050\nF1 Score for Tradition: 0.6872\nF1 Score for Universalism: concern: 0.2750\nF1 Score for Universalism: nature: 0.7714\nF1 Score for Universalism: tolerance: 0.5665\n0.505310909511586\n","output_type":"stream"}]},{"cell_type":"code","source":"torch.save(model, 'model_entailment.pth')","metadata":{"execution":{"iopub.status.busy":"2024-04-23T21:17:45.878673Z","iopub.execute_input":"2024-04-23T21:17:45.878946Z","iopub.status.idle":"2024-04-23T21:17:46.647314Z","shell.execute_reply.started":"2024-04-23T21:17:45.878924Z","shell.execute_reply":"2024-04-23T21:17:46.646482Z"},"trusted":true},"execution_count":31,"outputs":[]}]}